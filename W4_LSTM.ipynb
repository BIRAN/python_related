{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "W4_LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BIRAN/python_related/blob/master/W4_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1J15Vh_1Jih",
        "colab_type": "code",
        "outputId": "7151ce4a-0733-465d-b486-9d7055f107d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        }
      },
      "source": [
        "!pip install tf-nightly-2.0-preview\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-nightly-2.0-preview\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/51/250153713afcd836e4010464801719fb486af8ca6aa3abbe213bed677a08/tf_nightly_2.0_preview-2.0.0.dev20190803-cp36-cp36m-manylinux2010_x86_64.whl (86.2MB)\n",
            "\u001b[K     |████████████████████████████████| 86.2MB 1.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.15.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.0.8)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.11.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.7.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.2.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (3.7.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.33.4)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.16.4)\n",
            "Collecting tensorflow-estimator-2.0-preview (from tf-nightly-2.0-preview)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/b6/bff83560acf1082f539bcfcba4a4fe627988e9544d2e917788f83f50ea5e/tensorflow_estimator_2.0_preview-1.14.0.dev2019080300-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 43.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.1.7)\n",
            "Collecting opt-einsum>=2.3.2 (from tf-nightly-2.0-preview)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/d6/44792ec668bcda7d91913c75237314e688f70415ab2acd7172c845f0b24f/opt_einsum-2.3.2.tar.gz (59kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 22.9MB/s \n",
            "\u001b[?25hCollecting tb-nightly<1.16.0a0,>=1.15.0a0 (from tf-nightly-2.0-preview)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/ba/1704b725ebcd32919a111b7028574a6fd5058b285b031c0e03e598fb3cfc/tb_nightly-1.15.0a20190802-py3-none-any.whl (4.3MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3MB 34.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tf-nightly-2.0-preview) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tf-nightly-2.0-preview) (41.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.16.0a0,>=1.15.0a0->tf-nightly-2.0-preview) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.16.0a0,>=1.15.0a0->tf-nightly-2.0-preview) (0.15.5)\n",
            "Building wheels for collected packages: opt-einsum\n",
            "  Building wheel for opt-einsum (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for opt-einsum: filename=opt_einsum-2.3.2-cp36-none-any.whl size=49882 sha256=9569cfc12030997808689318aa36d1353bb4df464e51977e3f45ce5d980d209d\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/3e/a3/b351fae0cbf15373c2136a54a70f43fea5fe91d8168a5faaa4\n",
            "Successfully built opt-einsum\n",
            "Installing collected packages: tensorflow-estimator-2.0-preview, opt-einsum, tb-nightly, tf-nightly-2.0-preview\n",
            "Successfully installed opt-einsum-2.3.2 tb-nightly-1.15.0a20190802 tensorflow-estimator-2.0-preview-1.14.0.dev2019080300 tf-nightly-2.0-preview-2.0.0.dev20190803\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOjujz601HcS",
        "colab_type": "code",
        "outputId": "0604f3b8-0722-4a10-eabf-8b43a3187030",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0-dev20190803\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zswl7jRtGzkk",
        "colab": {}
      },
      "source": [
        "def plot_series(time, series, format=\"-\", start=0, end=None):\n",
        "    plt.plot(time[start:end], series[start:end], format)\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.grid(True)\n",
        "\n",
        "def trend(time, slope=0):\n",
        "    return slope * time\n",
        "\n",
        "def seasonal_pattern(season_time):\n",
        "    \"\"\"Just an arbitrary pattern, you can change it if you wish\"\"\"\n",
        "    return np.where(season_time < 0.4,\n",
        "                    np.cos(season_time * 2 * np.pi),\n",
        "                    1 / np.exp(3 * season_time))\n",
        "\n",
        "def seasonality(time, period, amplitude=1, phase=0):\n",
        "    \"\"\"Repeats the same pattern at each period\"\"\"\n",
        "    season_time = ((time + phase) % period) / period\n",
        "    return amplitude * seasonal_pattern(season_time)\n",
        "\n",
        "def noise(time, noise_level=1, seed=None):\n",
        "    rnd = np.random.RandomState(seed)\n",
        "    return rnd.randn(len(time)) * noise_level\n",
        "\n",
        "time = np.arange(4 * 365 + 1, dtype=\"float32\")\n",
        "baseline = 10\n",
        "series = trend(time, 0.1)  \n",
        "baseline = 10\n",
        "amplitude = 40\n",
        "slope = 0.05\n",
        "noise_level = 5\n",
        "\n",
        "# Create the series\n",
        "series = baseline + trend(time, slope) + seasonality(time, period=365, amplitude=amplitude)\n",
        "# Update with noise\n",
        "series += noise(time, noise_level, seed=42)\n",
        "\n",
        "split_time = 1000\n",
        "time_train = time[:split_time]\n",
        "x_train = series[:split_time]\n",
        "time_valid = time[split_time:]\n",
        "x_valid = series[split_time:]\n",
        "\n",
        "window_size = 20\n",
        "batch_size = 32\n",
        "shuffle_buffer_size = 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sTTIOCbyShY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(series)\n",
        "  dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
        "  dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1]))\n",
        "  dataset = dataset.batch(batch_size).prefetch(1)\n",
        "  return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1Hl39rklkLm",
        "colab_type": "code",
        "outputId": "ec29c8b3-36f1-41d3-952b-f1f72ca73b2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(51)\n",
        "np.random.seed(51)\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),\n",
        "                      input_shape=[None]),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "  tf.keras.layers.Dense(1),\n",
        "  tf.keras.layers.Lambda(lambda x: x * 100.0)\n",
        "])\n",
        "\n",
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
        "    lambda epoch: 1e-8 * 10**(epoch / 20))\n",
        "optimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\n",
        "model.compile(loss=tf.keras.losses.Huber(),\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"mae\"])\n",
        "history = model.fit(dataset, epochs=100, callbacks=[lr_schedule])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0804 01:31:47.184973 140223157118848 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/util/random_seed.py:58: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0804 01:31:51.257431 140223157118848 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:468: BaseResourceVariable.constraint (from tensorflow.python.ops.resource_variable_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Apply a constraint manually following the optimizer update step.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "31/31 [==============================] - 8s 258ms/step - loss: 21.4949 - mae: 22.0115\n",
            "Epoch 2/100\n",
            "31/31 [==============================] - 1s 35ms/step - loss: 21.2155 - mae: 21.6440\n",
            "Epoch 3/100\n",
            "31/31 [==============================] - 1s 34ms/step - loss: 20.7115 - mae: 21.2278\n",
            "Epoch 4/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 20.1586 - mae: 20.7739\n",
            "Epoch 5/100\n",
            "31/31 [==============================] - 1s 34ms/step - loss: 19.7903 - mae: 20.2843\n",
            "Epoch 6/100\n",
            "31/31 [==============================] - 1s 34ms/step - loss: 19.2600 - mae: 19.6776\n",
            "Epoch 7/100\n",
            "31/31 [==============================] - 1s 34ms/step - loss: 18.2675 - mae: 18.7308\n",
            "Epoch 8/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 17.4072 - mae: 17.9757\n",
            "Epoch 9/100\n",
            "31/31 [==============================] - 1s 34ms/step - loss: 17.2251 - mae: 17.6581\n",
            "Epoch 10/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 16.8969 - mae: 17.3457\n",
            "Epoch 11/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 16.6153 - mae: 17.0459\n",
            "Epoch 12/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 16.2141 - mae: 16.7529\n",
            "Epoch 13/100\n",
            "31/31 [==============================] - 1s 34ms/step - loss: 15.9663 - mae: 16.4573\n",
            "Epoch 14/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 15.6956 - mae: 16.1587\n",
            "Epoch 15/100\n",
            "31/31 [==============================] - 1s 35ms/step - loss: 15.3736 - mae: 15.8571\n",
            "Epoch 16/100\n",
            "31/31 [==============================] - 1s 36ms/step - loss: 15.1127 - mae: 15.5765\n",
            "Epoch 17/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 14.7840 - mae: 15.2950\n",
            "Epoch 18/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 14.4769 - mae: 15.0164\n",
            "Epoch 19/100\n",
            "31/31 [==============================] - 1s 34ms/step - loss: 14.3353 - mae: 14.7612\n",
            "Epoch 20/100\n",
            "31/31 [==============================] - 1s 34ms/step - loss: 14.0844 - mae: 14.5354\n",
            "Epoch 21/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 13.8134 - mae: 14.3081\n",
            "Epoch 22/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 13.6388 - mae: 14.1062\n",
            "Epoch 23/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 13.3828 - mae: 13.8858\n",
            "Epoch 24/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 13.1957 - mae: 13.7037\n",
            "Epoch 25/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 13.0152 - mae: 13.4526\n",
            "Epoch 26/100\n",
            "31/31 [==============================] - 1s 35ms/step - loss: 12.7582 - mae: 13.2327\n",
            "Epoch 27/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 12.5275 - mae: 12.9716\n",
            "Epoch 28/100\n",
            "31/31 [==============================] - 1s 32ms/step - loss: 12.2088 - mae: 12.7095\n",
            "Epoch 29/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 12.3934 - mae: 12.8460\n",
            "Epoch 30/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 11.8438 - mae: 12.3121\n",
            "Epoch 31/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 11.3302 - mae: 11.8148\n",
            "Epoch 32/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 10.9489 - mae: 11.4830\n",
            "Epoch 33/100\n",
            "31/31 [==============================] - 1s 34ms/step - loss: 10.6270 - mae: 11.1361\n",
            "Epoch 34/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 10.7821 - mae: 11.3007\n",
            "Epoch 35/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 10.9719 - mae: 11.4606\n",
            "Epoch 36/100\n",
            "31/31 [==============================] - 1s 35ms/step - loss: 10.4495 - mae: 10.9636\n",
            "Epoch 37/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 9.9901 - mae: 10.4892\n",
            "Epoch 38/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 9.5775 - mae: 10.0283\n",
            "Epoch 39/100\n",
            "31/31 [==============================] - 1s 34ms/step - loss: 9.0663 - mae: 9.5461\n",
            "Epoch 40/100\n",
            "31/31 [==============================] - 1s 34ms/step - loss: 8.6546 - mae: 9.1358\n",
            "Epoch 41/100\n",
            "31/31 [==============================] - 1s 34ms/step - loss: 8.2880 - mae: 8.7356\n",
            "Epoch 42/100\n",
            "31/31 [==============================] - 1s 32ms/step - loss: 7.9125 - mae: 8.4081\n",
            "Epoch 43/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 7.6783 - mae: 8.1137\n",
            "Epoch 44/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 7.3541 - mae: 7.8033\n",
            "Epoch 45/100\n",
            "31/31 [==============================] - 1s 34ms/step - loss: 7.1044 - mae: 7.5770\n",
            "Epoch 46/100\n",
            "31/31 [==============================] - 1s 35ms/step - loss: 6.9451 - mae: 7.3933\n",
            "Epoch 47/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 6.5533 - mae: 7.0771\n",
            "Epoch 48/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 6.4139 - mae: 6.9074\n",
            "Epoch 49/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 6.3932 - mae: 6.8757\n",
            "Epoch 50/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 6.0517 - mae: 6.5164\n",
            "Epoch 51/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 5.8718 - mae: 6.3634\n",
            "Epoch 52/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 5.7788 - mae: 6.2421\n",
            "Epoch 53/100\n",
            "31/31 [==============================] - 1s 34ms/step - loss: 5.6698 - mae: 6.1419\n",
            "Epoch 54/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 5.5064 - mae: 5.9468\n",
            "Epoch 55/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 5.4382 - mae: 5.9006\n",
            "Epoch 56/100\n",
            "31/31 [==============================] - 1s 34ms/step - loss: 5.2936 - mae: 5.7471\n",
            "Epoch 57/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 5.3198 - mae: 5.7881\n",
            "Epoch 58/100\n",
            "31/31 [==============================] - 1s 32ms/step - loss: 5.1549 - mae: 5.6591\n",
            "Epoch 59/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 5.3084 - mae: 5.7545\n",
            "Epoch 60/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 5.0298 - mae: 5.5567\n",
            "Epoch 61/100\n",
            "31/31 [==============================] - 1s 32ms/step - loss: 4.9550 - mae: 5.4250\n",
            "Epoch 62/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 5.1296 - mae: 5.5808\n",
            "Epoch 63/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 5.2112 - mae: 5.6630\n",
            "Epoch 64/100\n",
            "31/31 [==============================] - 1s 35ms/step - loss: 5.0575 - mae: 5.5312\n",
            "Epoch 65/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 4.9258 - mae: 5.3900\n",
            "Epoch 66/100\n",
            "31/31 [==============================] - 1s 34ms/step - loss: 5.3043 - mae: 5.7637\n",
            "Epoch 67/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 5.2753 - mae: 5.7446\n",
            "Epoch 68/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 5.2993 - mae: 5.7234\n",
            "Epoch 69/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 5.3742 - mae: 5.8346\n",
            "Epoch 70/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 6.1359 - mae: 6.6386\n",
            "Epoch 71/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 4.9126 - mae: 5.4135\n",
            "Epoch 72/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 5.6095 - mae: 6.0636\n",
            "Epoch 73/100\n",
            "31/31 [==============================] - 1s 32ms/step - loss: 5.3411 - mae: 5.8208\n",
            "Epoch 74/100\n",
            "31/31 [==============================] - 1s 32ms/step - loss: 6.6834 - mae: 7.1966\n",
            "Epoch 75/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 5.7113 - mae: 6.1957\n",
            "Epoch 76/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 5.1433 - mae: 5.6158\n",
            "Epoch 77/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 5.7382 - mae: 6.2385\n",
            "Epoch 78/100\n",
            "31/31 [==============================] - 1s 34ms/step - loss: 7.2282 - mae: 7.6426\n",
            "Epoch 79/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 5.0664 - mae: 5.5955\n",
            "Epoch 80/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 6.4776 - mae: 7.0136\n",
            "Epoch 81/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 6.6766 - mae: 7.1079\n",
            "Epoch 82/100\n",
            "31/31 [==============================] - 1s 32ms/step - loss: 6.7007 - mae: 7.2374\n",
            "Epoch 83/100\n",
            "31/31 [==============================] - 1s 34ms/step - loss: 6.3450 - mae: 6.8347\n",
            "Epoch 84/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 5.8163 - mae: 6.3277\n",
            "Epoch 85/100\n",
            "31/31 [==============================] - 1s 34ms/step - loss: 7.0088 - mae: 7.6007\n",
            "Epoch 86/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 7.4649 - mae: 7.9559\n",
            "Epoch 87/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 8.0526 - mae: 8.5456\n",
            "Epoch 88/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 7.0569 - mae: 7.4773\n",
            "Epoch 89/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 6.4203 - mae: 6.9714\n",
            "Epoch 90/100\n",
            "31/31 [==============================] - 1s 34ms/step - loss: 7.9175 - mae: 8.3901\n",
            "Epoch 91/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 5.9646 - mae: 6.4224\n",
            "Epoch 92/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 7.6886 - mae: 8.2557\n",
            "Epoch 93/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 10.2000 - mae: 10.6407\n",
            "Epoch 94/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 9.8386 - mae: 10.2520\n",
            "Epoch 95/100\n",
            "31/31 [==============================] - 1s 34ms/step - loss: 8.0082 - mae: 8.6051\n",
            "Epoch 96/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 10.0811 - mae: 10.7728\n",
            "Epoch 97/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 12.3814 - mae: 12.8968\n",
            "Epoch 98/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 11.2206 - mae: 11.6217\n",
            "Epoch 99/100\n",
            "31/31 [==============================] - 1s 34ms/step - loss: 14.2741 - mae: 14.6141\n",
            "Epoch 100/100\n",
            "31/31 [==============================] - 1s 33ms/step - loss: 10.7613 - mae: 11.1881\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkBsrsXMzoWR",
        "colab_type": "code",
        "outputId": "41556269-ec15-4de2-a05a-286103edaae5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        }
      },
      "source": [
        "plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n",
        "plt.axis([1e-8, 1e-4, 0, 30])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1e-08, 0.0001, 0, 30]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nX0TEQ/4PpUQ0+8caPg0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uh-97bpLZCA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(51)\n",
        "np.random.seed(51)\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),\n",
        "                      input_shape=[None]),\n",
        "   tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "  tf.keras.layers.Dense(1),\n",
        "  tf.keras.layers.Lambda(lambda x: x * 100.0)\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(lr=1e-5, momentum=0.9),metrics=[\"mae\"])\n",
        "history = model.fit(dataset,epochs=500,verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icGDaND7z0ne",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "forecast = []\n",
        "results = []\n",
        "for time in range(len(series) - window_size):\n",
        "  forecast.append(model.predict(series[time:time + window_size][np.newaxis]))\n",
        "\n",
        "forecast = forecast[split_time-window_size:]\n",
        "results = np.array(forecast)[:, 0, 0]\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "plot_series(time_valid, x_valid)\n",
        "plot_series(time_valid, results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfPeqI7rz4LD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.metrics.mean_absolute_error(x_valid, results).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUsdZB_tzDLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.image  as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#-----------------------------------------------------------\n",
        "# Retrieve a list of list results on training and test data\n",
        "# sets for each training epoch\n",
        "#-----------------------------------------------------------\n",
        "mae=history.history['mae']\n",
        "loss=history.history['loss']\n",
        "\n",
        "epochs=range(len(loss)) # Get number of epochs\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot MAE and Loss\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs, mae, 'r')\n",
        "plt.plot(epochs, loss, 'b')\n",
        "plt.title('MAE and Loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend([\"MAE\", \"Loss\"])\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "epochs_zoom = epochs[200:]\n",
        "mae_zoom = mae[200:]\n",
        "loss_zoom = loss[200:]\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot Zoomed MAE and Loss\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs_zoom, mae_zoom, 'r')\n",
        "plt.plot(epochs_zoom, loss_zoom, 'b')\n",
        "plt.title('MAE and Loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend([\"MAE\", \"Loss\"])\n",
        "\n",
        "plt.figure()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CGaYFxXNEAK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),\n",
        "                      input_shape=[None]),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "  tf.keras.layers.Dense(1),\n",
        "  tf.keras.layers.Lambda(lambda x: x * 100.0)\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9))\n",
        "model.fit(dataset,epochs=100, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJ3R8ysauz9e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),\n",
        "                      input_shape=[None]),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "  tf.keras.layers.Dense(1),\n",
        "  tf.keras.layers.Lambda(lambda x: x * 100.0)\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9))\n",
        "model.fit(dataset,epochs=100)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}